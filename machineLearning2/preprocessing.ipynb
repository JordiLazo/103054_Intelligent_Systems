{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d918bb88-9075-40ad-b21f-28b72e86c487",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Preprocessing\n",
    "This notebook will walk through the concepts we saw today. Specifically, it presents\n",
    "\n",
    "- How to load data using pandas\n",
    "- How to split the data into train and test\n",
    "- How to use the differents transformers\n",
    "- A complete example on how to preprocess the data on a real dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d28059-e464-4a7b-bb45-5be805ed1c44",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8053d4d-fee0-48ae-a717-80cb2e722b07",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "First of all, we will see how to load data using **pandas**. We will see 2 examples: loading a local file and loading a dataset from a URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73335c54-78e1-4e65-8aa4-e594ef610856",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "titanic_from_file = pandas.read_csv(\"titanic.csv\")\n",
    "titanic_from_url = pandas.read_csv(\"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0101ad32-b22d-4c2b-bdc2-e1eae5178c6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_from_file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a74c079f-c009-4238-9f6e-29aaa34e02fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_from_url.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb534f57-cf0b-4cd8-8d6a-142d330efe64",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Split the data into train and test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef29ba0-9a45-459f-9120-940b814a9700",
   "metadata": {},
   "source": [
    "\n",
    "The next step once we have the data loaded into memory using **pandas** is to obtain our train and test datasets. Remember that this process uses random selection, so for reproducibility we will set a seed. This means that each time we call this method it will return the same selection.\n",
    "\n",
    "First, we will separate the data into features (X) and labels (y), as this dataset is usually used for classification (i.e. supervised learning), and contains a column with the label *Survived*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b8424b0-47d6-481a-b854-37cd992f31df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use drop to get the dataset without a specific column. If the parameter\n",
    "# inplace is not set to True, it will not modify the original data\n",
    "X = titanic_from_file.drop(\"Survived\", axis=1)\n",
    "y = titanic_from_file[\"Survived\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a396135f-51ac-4ce4-b260-5cb3d27bb978",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Pclass                                               Name  \\\n",
       "0            1       3                            Braund, Mr. Owen Harris   \n",
       "1            2       1  Cumings, Mrs. John Bradley (Florence Briggs Th...   \n",
       "2            3       3                             Heikkinen, Miss. Laina   \n",
       "3            4       1       Futrelle, Mrs. Jacques Heath (Lily May Peel)   \n",
       "4            5       3                           Allen, Mr. William Henry   \n",
       "\n",
       "      Sex   Age  SibSp  Parch            Ticket     Fare Cabin Embarked  \n",
       "0    male  22.0      1      0         A/5 21171   7.2500   NaN        S  \n",
       "1  female  38.0      1      0          PC 17599  71.2833   C85        C  \n",
       "2  female  26.0      0      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3  female  35.0      1      0            113803  53.1000  C123        S  \n",
       "4    male  35.0      0      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03113db6-a2d4-4df1-974b-9e4f200b3f95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    1\n",
       "2    1\n",
       "3    1\n",
       "4    0\n",
       "Name: Survived, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2fe8cf-7bd1-46ae-b9c8-2640edcdbd57",
   "metadata": {},
   "source": [
    "Now we are ready to split the data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd186c06-a014-43ff-a365-824fdff37ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(891, 11) (712, 11) (179, 11)\n",
      "(891,) (712,) (179,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "test_size_pctg = 0.2\n",
    "seed = 42\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size_pctg, random_state=seed)\n",
    "\n",
    "print(X.shape, X_train.shape, X_test.shape)\n",
    "print(y.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19b9ad2-c7ee-4a26-987a-53fc3af936f6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Transforming data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bfc14e-237f-477d-885d-b50f12933949",
   "metadata": {},
   "source": [
    "\n",
    "Here we show a basic example of the transformers explained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1246598f-b5b3-493c-9a3f-3b5605bf707a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6644d2ec-359c-497c-9f68-301b1f7b723e",
   "metadata": {
    "tags": []
   },
   "source": [
    "First, we see the normalization process using the `StandardScaler`. This will convert each column into a new column with standard deviation of 1 and mean of 0.\n",
    "\n",
    "As the standard scaler will apply the function:\n",
    "$$\n",
    "    z_i = \\frac{x_i - \\mu}{\\sigma}\n",
    "$$\n",
    "to scale the values, first it must learn this $\\mu$ (mean) and $\\sigma$ (std. deviation). To learn this values (parameters) we will **fit** the transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5f7622c-cee6-443f-a142-1cd35d691962",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column1</th>\n",
       "      <th>column2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.444444</td>\n",
       "      <td>3.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.185813</td>\n",
       "      <td>2.619372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        column1   column2\n",
       "count  9.000000  9.000000\n",
       "mean   3.444444  3.111111\n",
       "std    2.185813  2.619372\n",
       "min    1.000000 -1.000000\n",
       "25%    2.000000  1.000000\n",
       "50%    3.000000  3.000000\n",
       "75%    5.000000  5.000000\n",
       "max    7.000000  7.000000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pandas.DataFrame({\n",
    "    \"column1\": [1, 2, 4, 5, 6, 7, 2, 1, 3],\n",
    "    \"column2\": [-1, 2, 1, 1, 3, 4, 5, 6, 7],\n",
    "})\n",
    "\n",
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b75d2a65-4aea-46fa-8f20-666b39b279f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.18616051, -1.6647087 ],\n",
       "       [-0.70091303, -0.44992127],\n",
       "       [ 0.26958193, -0.85485041],\n",
       "       [ 0.75482941, -0.85485041],\n",
       "       [ 1.24007689, -0.04499213],\n",
       "       [ 1.72532437,  0.35993702],\n",
       "       [-0.70091303,  0.76486616],\n",
       "       [-1.18616051,  1.1697953 ],\n",
       "       [-0.21566555,  1.57472445]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(dataset)  # Learn std.dev and mean\n",
    "\n",
    "new_X = scaler.transform(dataset)\n",
    "# Note that the scaler will return a numpy array\n",
    "new_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "75db0944-d70e-4131-a432-4ffd1c766fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean for each column: [-1.04854397e-16  0.00000000e+00]\n",
      "Standard deviation for each column: [1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean for each column:\", new_X.mean(axis=0))\n",
    "print(\"Standard deviation for each column:\", new_X.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad408f47-27fe-49cd-b2af-f50082df8ca0",
   "metadata": {},
   "source": [
    "Finally, let's see that this transformer allows us to recover the original data using `inverse_transform`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c53f712-032f-4566-b829-662ae70ab932",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1., -1.],\n",
       "       [ 2.,  2.],\n",
       "       [ 4.,  1.],\n",
       "       [ 5.,  1.],\n",
       "       [ 6.,  3.],\n",
       "       [ 7.,  4.],\n",
       "       [ 2.,  5.],\n",
       "       [ 1.,  6.],\n",
       "       [ 3.,  7.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.inverse_transform(new_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6777e4-460c-4738-ae11-d0c75852bfb2",
   "metadata": {},
   "source": [
    "As an exercise, try to implement the StandardScaler by yourself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d0e72610-dd01-4853-a4ac-8912cba568ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 24\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[39m# Use the mean and the std deviation of each column to scale\u001b[39;00m\n\u001b[1;32m     21\u001b[0m         \u001b[39m# the data in X\u001b[39;00m\n\u001b[1;32m     22\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m my_X \u001b[39m=\u001b[39m MyStandardScaler()\u001b[39m.\u001b[39;49mfit_transform(dataset)\n",
      "File \u001b[0;32m~/git/Intelligent_Systems/venv/lib/python3.8/site-packages/sklearn/utils/_set_output.py:142\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[1;32m    141\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 142\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, X, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    143\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    144\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    145\u001b[0m         \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    146\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[1;32m    147\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[1;32m    148\u001b[0m         )\n",
      "File \u001b[0;32m~/git/Intelligent_Systems/venv/lib/python3.8/site-packages/sklearn/utils/_set_output.py:142\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[1;32m    141\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 142\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, X, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    143\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    144\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    145\u001b[0m         \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    146\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[1;32m    147\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[1;32m    148\u001b[0m         )\n",
      "File \u001b[0;32m~/git/Intelligent_Systems/venv/lib/python3.8/site-packages/sklearn/base.py:848\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    844\u001b[0m \u001b[39m# non-optimized default implementation; override when a better\u001b[39;00m\n\u001b[1;32m    845\u001b[0m \u001b[39m# method is possible for a given clustering algorithm\u001b[39;00m\n\u001b[1;32m    846\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    847\u001b[0m     \u001b[39m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[0;32m--> 848\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(X, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\u001b[39m.\u001b[39mtransform(X)\n\u001b[1;32m    849\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    850\u001b[0m     \u001b[39m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m    851\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n",
      "Cell \u001b[0;32mIn[19], line 9\u001b[0m, in \u001b[0;36mMyStandardScaler.fit\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(X, pandas\u001b[39m.\u001b[39mDataFrame):\n\u001b[1;32m      7\u001b[0m     X \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mvalues\n\u001b[0;32m----> 9\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39m# TODO get the mean and the standard deviation for each column\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmeans_ \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class MyStandardScaler(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X):\n",
    "        if isinstance(X, pandas.DataFrame):\n",
    "            X = X.values\n",
    "        \n",
    "        raise NotImplementedError\n",
    "        # TODO get the mean and the standard deviation for each column\n",
    "        self.means_ = None\n",
    "        self.std_dev_ = None\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        if isinstance(X, pandas.DataFrame):\n",
    "            X = X.values\n",
    "        \n",
    "        # Use the mean and the std deviation of each column to scale\n",
    "        # the data in X\n",
    "        raise NotImplementedError\n",
    "        \n",
    "my_X = MyStandardScaler().fit_transform(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432adf6e-a911-4651-9fda-5f1dd1b0aaaf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Discretization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5d5da3-3869-4d2c-af2e-f4b13bd71fdd",
   "metadata": {},
   "source": [
    "The next transformer we explained is the *bins discretizer*. Experiment with the different parameters available (see [KBinsDiscretizer](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.KBinsDiscretizer.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50abaf18-6b8a-4189-b04a-771d0a7a455a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "discretizer = KBinsDiscretizer(n_bins=3, encode=\"ordinal\")\n",
    "discretizer.fit_transform(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0972b7-e8fc-4e1e-8306-953cf6df94ff",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abe3315-4f33-4458-bd0b-4791bbfb1c2e",
   "metadata": {},
   "source": [
    "Finally, let's see the encoders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95661fe-65b3-42c1-8038-9fcd9e2ee838",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ohe = OneHotEncoder(sparse=False)\n",
    "print(ohe.fit_transform(dataset))\n",
    "print(ohe.categories_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e7a1b5-7e31-4ea1-87f3-a193d14bdbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "dataset_categorical = pandas.DataFrame({\n",
    "    \"col1\": [\"cat\", \"dog\", \"bird\", \"cat\", \"bird\", \"cat\"],\n",
    "})\n",
    "\n",
    "label_enc = LabelEncoder()\n",
    "print(label_enc.fit_transform(dataset_categorical))\n",
    "print(label_enc.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea5111e-630d-4064-8f8e-8ee367931d4c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6f217d-c39a-4b43-9c5a-bf44a79838c5",
   "metadata": {},
   "source": [
    "\n",
    "In some cases, we might not be able to determine which features are worth or not to be used in a model manually. For those cases, we can use statistics and unsupervised learning to find relations between the features and the label in our training set, and then select some features based on this new information.\n",
    "\n",
    "An example of this technique is the SelectKBest method in sklearn, where we determine which are the best $K$ features based on univariate statistical tests, such as $\\chi^2$ (chi squared)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1de290a-b160-4fdb-a927-9f318076fe4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "print(X.shape)\n",
    "\n",
    "selector = SelectKBest(chi2, k=2)\n",
    "X_new = selector.fit_transform(X, y)\n",
    "\n",
    "print(X_new.shape)\n",
    "print(selector.scores_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673a8831-1c0f-4abd-959c-c3d18b8dd8f5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data scrubbing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58139908-8c5b-4280-b348-3109a4de3921",
   "metadata": {},
   "source": [
    "Below we have a real example on how to preprocess a dataset. We will use the[Titanic dataset](https://hbiostat.org/data/repo/titanic.txt).\n",
    "\n",
    "The problem we try to solve is to determine if a Titanic's passenger would have survived, given the age, passenger class, and sex.\n",
    "\n",
    "The attributes are:\n",
    "- idx (we use it as the dataframe index using index_col=0)\n",
    "- Class\n",
    "- Survived (1=True / 0=False)\n",
    "- Name\n",
    "- Age\n",
    "- Embarked\n",
    "- Destination\n",
    "- Room\n",
    "- Ticket\n",
    "- Boat\n",
    "- Sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7719ba39-06f6-480b-ac25-6735b53af46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "titanic = pandas.read_csv(\"https://hbiostat.org/data/repo/titanic.txt\", index_col=0)\n",
    "\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6aa719-7e0e-40f7-b0cf-eb2d56ad38e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_X = titanic.drop(\"survived\", axis=1)\n",
    "titanic_y = titanic[\"survived\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    titanic_X, titanic_y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052d8be7-f587-4f2f-a735-61940d86b6cb",
   "metadata": {},
   "source": [
    "Let's select some attributes we will use for learning:\n",
    "- Passenger class (pclass)\n",
    "- Age\n",
    "- Sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f618608-a9b9-403a-81df-29a7393ee433",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[[\"pclass\", \"age\", \"sex\"]]\n",
    "\n",
    "print(X_train.shape)\n",
    "X_train.isna().sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1d4f0d-811c-4f13-a2ed-134938f72048",
   "metadata": {},
   "source": [
    "We see that for the age attribute we have 448 rows of 1050 that have missing values. We can approach this using any of the techniques we explained:\n",
    "- Replacing it with the mean value\n",
    "- Replacing it using KNN\n",
    "- Replacing it randomly\n",
    "- Deleting the records\n",
    "\n",
    "The example uses the SimpleImputer to show how this can be done, but as we know it is easy to replace one transformer for another using *sklearn*, so try to change it and experiment with the other methods. Remember that other methods might require the data as integers only. In those cases, for now, you can apply the fit and transform only to a specific column using something like:\n",
    "```python\n",
    "dest = titanic_X.copy()\n",
    "columns_to_input = [\"age\"]\n",
    "dest[columns_to_input] = imputer.fit_transform(titanic_X[columns_to_input] \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770d5618-54f7-4caa-8abb-19d4c2a999ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "\n",
    "# First learn the parameters (in the simple imputer this is the\n",
    "# most frequent value) for each column \n",
    "imputer.fit(X_train)\n",
    "# Then \"impute\" the values\n",
    "X_train_imputed = imputer.transform(X_train)\n",
    "\n",
    "# \"Reconvert\" it to a dataframe\n",
    "X_train_imputed = pandas.DataFrame(X_train_imputed,\n",
    "                                   columns=X_train.columns,\n",
    "                                   index=X_train.index)\n",
    "\n",
    "# Check we have no more nans\n",
    "print(X_train_imputed.shape)\n",
    "X_train_imputed.isna().sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919a87ed-45fa-49bf-a923-e9b5beb8e7ab",
   "metadata": {},
   "source": [
    "Once we have the values imputed, then we can prepare it for our model. We will use sklearn decision trees. This model expects as input a list of *real-valued* features, as the decisions will be **Feature <= value**.\n",
    "\n",
    "As we have categorical data, we have to convert it to integer or real values. We could use either a *LabelEncoder* or a *OneHotEncoder*.\n",
    "\n",
    "The example shows a *LabelEncoder* for the feature *pclass*, but remember that as the category might not have a natural ordering, a *OneHotEncoder* might be better as the *LabelEncoder* will map each category to an integer between $[0, K-1]$, where $K$ is the number of categories for each column, thus introducing an ordering between classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff897019-f159-48f2-9dfd-34a9591033cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "\n",
    "X_train_encoded = X_train_imputed.copy()\n",
    "\n",
    "pclass_encoder = LabelEncoder()\n",
    "pclass_encoder = pclass_encoder.fit(X_train_imputed[\"pclass\"])\n",
    "X_train_encoded[\"pclass\"] = pclass_encoder.transform(X_train_imputed[\"pclass\"])\n",
    "\n",
    "# [...] Exercise: encode the age using a OneHotEncoder (set the parameter sparse to false)\n",
    "# Remember that the OHE works with multiple columns as input, while the LabelEncoder\n",
    "# works with a single column input, so the calls to fit and transform might be a bit\n",
    "# different.\n",
    "# Hint: as the OneHotEncoder will return mutliple columns for each column\n",
    "# you can drop the original column in X_train_encoded and use something like:\n",
    "# >>> new_columns = list(map(lambda s: \"sex_\" + s, list(encoder.categories_[0])))\n",
    "# >>> X_train_encoded[new_columns] = encoder.transform(...)\n",
    "\n",
    "X_train_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9b367f-2274-4af0-9a81-e248dcb9c732",
   "metadata": {},
   "source": [
    "We are now ready to fit our first model with clean data. We first separate it into train and test, and then fit the classifier. Finally, we check the score of the model in both train and test, to see if our model overfits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f84b641-9c8c-4cd9-b794-a935c98d3411",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "# NOTE: Once you have encoded the column sex using OHE, you can comment the following line\n",
    "X_train_encoded = X_train_encoded[[\"pclass\", \"age\"]]\n",
    "\n",
    "\n",
    "model = DecisionTreeClassifier(criterion=\"entropy\", max_depth=3, min_samples_leaf=5)\n",
    "model.fit(X_train_encoded, y_train)\n",
    "\n",
    "print(\"Score in train:\", model.score(X_train_encoded, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a507d8c8-2215-4952-9417-5f6053a1db13",
   "metadata": {},
   "source": [
    "In order to get the score for the test dataset, we have to apply the same preprocessing as we dit to the train dataset. **Remember that the test dataset cannot modify any parameter of the entire process. This means that we cannot call any method that fits anything**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b3a9014-d874-4719-b735-d68fe44e4d0d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['pclass', 'age', 'sex'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# TODO preprocess the dataset\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m X_test \u001b[39m=\u001b[39m X_test[[\u001b[39m\"\u001b[39;49m\u001b[39mpclass\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mage\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39msex\u001b[39;49m\u001b[39m\"\u001b[39;49m]]\n\u001b[1;32m      4\u001b[0m \u001b[39m# apply data imputer\u001b[39;00m\n\u001b[1;32m      5\u001b[0m X_test_prepared \u001b[39m=\u001b[39m imputer\u001b[39m.\u001b[39mtransform(X_test_prepared)\n",
      "File \u001b[0;32m~/git/Intelligent_Systems/venv/lib/python3.8/site-packages/pandas/core/frame.py:3811\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3809\u001b[0m     \u001b[39mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   3810\u001b[0m         key \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(key)\n\u001b[0;32m-> 3811\u001b[0m     indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49m_get_indexer_strict(key, \u001b[39m\"\u001b[39;49m\u001b[39mcolumns\u001b[39;49m\u001b[39m\"\u001b[39;49m)[\u001b[39m1\u001b[39m]\n\u001b[1;32m   3813\u001b[0m \u001b[39m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   3814\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(indexer, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39m==\u001b[39m \u001b[39mbool\u001b[39m:\n",
      "File \u001b[0;32m~/git/Intelligent_Systems/venv/lib/python3.8/site-packages/pandas/core/indexes/base.py:6113\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6110\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   6111\u001b[0m     keyarr, indexer, new_indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6113\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[1;32m   6115\u001b[0m keyarr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtake(indexer)\n\u001b[1;32m   6116\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6117\u001b[0m     \u001b[39m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/git/Intelligent_Systems/venv/lib/python3.8/site-packages/pandas/core/indexes/base.py:6173\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6171\u001b[0m     \u001b[39mif\u001b[39;00m use_interval_msg:\n\u001b[1;32m   6172\u001b[0m         key \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(key)\n\u001b[0;32m-> 6173\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNone of [\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m] are in the [\u001b[39m\u001b[39m{\u001b[39;00maxis_name\u001b[39m}\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   6175\u001b[0m not_found \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[39m.\u001b[39mnonzero()[\u001b[39m0\u001b[39m]]\u001b[39m.\u001b[39munique())\n\u001b[1;32m   6176\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mnot_found\u001b[39m}\u001b[39;00m\u001b[39m not in index\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['pclass', 'age', 'sex'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "X_test_prepared = X_test\n",
    "# TODO preprocess the dataset\n",
    "X_test = X_test[[\"pclass\", \"age\", \"sex\"]]\n",
    "\n",
    "# apply data imputer\n",
    "X_test_prepared = imputer.transform(X_test_prepared)\n",
    "X_test_prepared = pandas.DataFrame(X_test_prepared,\n",
    "                                   columns=X_test.columns,\n",
    "                                   index=X_test.index)\n",
    "# label encoder\n",
    "X_test_prepared[\"pclass\"] = pclass_encoder.transform(\n",
    "    X_test_prepared[\"pclass\"])\n",
    "\n",
    "# apply one hot encoder\n",
    "transformed_data = ohe.fit_transform(X_test_prepared[[\"sex\"]])\n",
    "X_test_prepared[new_columns] = transformed_data\n",
    "X_test_prepared.drop(columns=[\"sex\"], inplace=True)\n",
    "\n",
    "print(\"Score in test:\", model.score(X_test_prepared, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bfa27b-2d7a-4df4-a5d9-728ae9866aae",
   "metadata": {},
   "source": [
    "As a bonus, we will inspect the built tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9123df9-80a3-44c6-b853-0439a32a31f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "tree.plot_tree(model, feature_names=X_train.columns)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "a1d456c2b579bb6c143c03429e9b27db6575c292ffb89068bb038e9eec5a266d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
